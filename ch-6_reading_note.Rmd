---
title: "ch-6_reading_note <Linear Model Selection and Regularization>"
author: "Wendy"
date: "July 24, 2016"
output: html_document
---

##Chapter Purpose: 
####Prediction Accuracy: 
1. n >> p: the LEAST SQUARES estimates tend to also have low variance, and hence will perform well on test observation
2. n not >> p: a lot of variability in the LEAST SQUARES fit --> overfitting --> poor prediction 
3. n < p: the variance is infinite so the LEAST SQUARES cannot be used at all. By constraining or shrinking the estimated coef, we can often substantially reduce the variance at the cost of a negligible increase of bias. (Not sure how to make estimation from this condition. )

####Model Interpretability: 
| Methods | Select Judgement | Fault | Merit | R_function |
| --- | --- | --- | --- | --- |
| Subset Selection | the most important: low test error. others: CV prediction error, C<sub>p</sub>, BIC, or ajusted R<sup>2</sup> | computational limitations: the number of possible models that must be considered grwos rapidly as *p* increases.It becomes computationally infeasible for values of p greater than 40 | simple and conceptually appealing approach | regsubsets()/ *leaps library*/ use summary() could return R<sup>2</sup>, RSS, adjusted R<sup>2</sup>, C<sub>p</sub>, BIC  |
| Shrinkage | --- | --- | --- | --- |
| Dimension Reduction | --- | --- | --- | --- |

####(w 6.1)Choose Optimal Model with C<sub>p</sub>, AIC, BIC and Ajusted R<sup>2</sup>: 
####<span style="color:orange"> Skipped for now. </span>  
| Item | --- | --- |--- | --- |
| C<sub>p</sub> | --- | --- | --- | --- |
| AIC (Akaike information criterion) | --- | --- | --- | --- |
| BIC (Bayesian information criterion) | --- | --- | --- | --- |
| Adjusted R<sup>2</sup> | --- | --- | --- | --- |

####(w 6.2)Shrinkage Methods: 
| Item | Explanation | Shrinkage Penalty | Important | Note | Benefit | Disadvantage | Work Best Situation |
| Ridge Regression (page 229)| similar to least squares, except that the coef are estimated by minimizing a slightly different quantity. <span style="color:orange"> check the 6.5, page 229 </span> | the second term: lambda = 0, penalty term no effect; lambda -> infinite, the penalty increase, and the ridge regression coef estimates will approach zero. | select a good value for lambda | apply ridge regression after standardizing the predictors, because scale equivariant will also impact the value of X<sub>j</sub>Beta<sub>j</sub>/ formula 6.6 page 231 | bias-variance trade-off: lambda increase --> the flexibility of the ridge regression fit decrease --> decrease variance increase bias --> impact the test mean squared error mean // computation advantages over best subset selection | include all p predictors, which is not a problem for prediction accuracy, but create a challenge in model interpretation | the relationship between the response and the predictors is close to linear. least squares estimations have high variance but low bias, like: p > n or p is almost as large as n |
| Lasso (page 233) | ell 1 penalty has the effect of forcing some of coef estimates to be exactly to zero | --- | --- | --- | --- | overcome Ridge's disadvantage | --- |

####Another fomulation: 
when p = 2,   
the lasso coef estimates have the smallest RSS out of all points that lie within the diamond defined by |beta<sub>1</sub>| + |beta<sub>2</sub>| <= s.   
the ridge regression estimates have the smallest RSS out of all points that lie within the circle defined by beta<sub>1</sub><sup>2</sup> + beta<sub>2</sub><sup>2</sup> <= s.  
Note: if s is large enough, coef is just as the least squares; if s is small enough, variable selection.

###6.1 Subset Selection<Page 219>
####1. Best Subset Selection
####from page 219 lab from 258   

Algorithm 6.1
![Algorithm 6.1](photo_insert/ch-6.1_best_subset_selection_.png)

**Lab.6.5.1: Best Subset Selection**
*purpose:* to practice the best subset selection  
*method:* regsubsets()/ in library(leaps)   
*result:*  
```{r}
library(ISLR)
names(Hitters)
sum(is.na(Hitters))
Hitters = na.omit(Hitters)

library(leaps)
regfit.full = regsubsets(Salary ~ ., Hitters)
dim(Hitters)
summary(regfit.full)

regfit.full = regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
reg.summary = summary(regfit.full)

names(reg.summary)
```

My ans --- Lab asked to plot the r2, ajstr2, bic, etc. --- But seems something wrong. 
```{r}
regfit.full_plot = data.frame("x" = c(1:19), "r2" = reg.summary$rsq, "adjr2" = reg.summary$adjr2, "cp" = reg.summary$cp, "bic" = reg.summary$cp)

plot(regfit.full_plot$x, regfit.full_plot$r2, type = "l", xlab = "predictor number", ylab = "error rate", ylim)

lines(regfit.full_plot$x, regfit.full_plot$adjr2, type = "l",col = "red")

lines(regfit.full_plot$x, regfit.full_plot$cp, type = "l",col = "blue")

lines(regfit.full_plot$x, regfit.full_plot$bic, type = "l",col = "green")

regfit.full_plot$cp

```

lab's ans --- Lab asked to plot the r2, ajstr2, bic, etc.   
*Wendy's Comment* not need to xlab data, totally different than I thought before. =(
```{r}
par(mfrow = c(2, 2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")

plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSQ", type = "l")
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)

plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "red", cex = 2, pch = 20)

plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "red", cex = 2, pch = 20)
```

**Lab.6.5.2: Forward and Backward Stepwise Selection**
*purpose:* to practice the forward and backward stepwise  
*method:* regsubsets(... method = 'forward'/ 'backward') library(leaps)  
*result:*  
```{r}
library(leaps)
regfit.fwd = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = 'forward')
summary(regfit.fwd)
regfit.bwd = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = 'backward')
summary(regfit.bwd)
```


**Lab.6.5.3: Forward and Backward Stepwise Selection**
*purpose:* to practice the forward and backward stepwise  
*method:* regsubsets(... method = 'forward'/ 'backward') library(leaps)  
*result:*  

context: 6.1.3 choose the optimal model --> page 224 Cp, AIC, BIC and xxx
before page 261. about the Cp, aic, bic, etc, lab should be already talked about them in the Lab 6.5.1 and 6.5.2. 
####Current skip the rest of the 6.1. and go into the 6.2, page 228