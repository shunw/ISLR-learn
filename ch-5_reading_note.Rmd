---
title: "ch-5_reading_note"
author: "Wendy"
date: "June 29, 2016"
output: html_document
---

| Type | Subtype | Method | Merit | Demerit |
| --- | --- | --- | --- | --- |
| Cross-Validation | --- | --- | --- | ---|
| 1 | valid set | split the data into half, one is training data, the other is test data | --- | --- |
| 2 | Leave-One-Out CV | a single observation is used for validation | unbiased due to more data than item 1 - valid set/ no randomness in the training or validation set splits. ==> run LOOCV multiple times will yield the same results.  | highly variable (due to single observation) |


###Cross-Validation
####1. Valid Set
####from page 190 lab from 205 

**example @ 5.1.1**
*purpose:* to check which regression function is better  
*method:* to make split the dataset into two part.   
*result:* the poly(horsepower, 2) show more lower error rate. 
```{r}
library(ISLR)
set.seed(1)
set.seed(2)
train_index = sample(c(TRUE, FALSE), size = nrow(Auto), replace = TRUE, prob = c(.5, .5))

# make the plot for n = 1:10 error rate with same train_index
make_poly_1st <- function(n, train_index){
  temp.test_error <- NULL
  poly_num <- NULL
  
  for (i in 1:n) {
    temp.lm = lm(mpg ~ poly(horsepower, i), data = Auto, subset = train_index)
    temp.lm.pre = predict(temp.lm, Auto[!train_index, ])
    temp.test_error <- c(temp.test_error, mean((Auto[!train_index, "mpg"] - temp.lm.pre)^2))
    poly_num <- c(poly_num, i)
    
  }
  
  poly_test_err <- data.frame("poly" = poly_num, "test_err" = temp.test_error)
  return (poly_test_err)
}

# for 10 different train_index, check the n=1:10 error rate. 
make_poly_10 <- function(){
  for (i in 1:10){
    set.seed(i)
    train_index <- sample(c(TRUE, FALSE), size = nrow(Auto), replace = TRUE, prob = c(.5, .5))
    if (i == 1)
      {
      temp_data <- make_poly_1st(10, train_index)
      plot(test_err ~ poly, data = temp_data, type = "b", ylim = range(15, 28))
      # print (head(temp_data))
      }
    else
      {
      temp_data <- make_poly_1st(10, train_index)
      lines(temp_data$test_err, type = "b", col = i)
      # print (head(temp_data))
      }
    
  }
}

make_poly_10()
```

####2. Leave One Out CV 
#####from page 192

**example @ 5.1.2**
*purpose:* to valid the figure 5.4
*method:* 
*result:* 
__STOP IN THE PAGE 194, NEXT STEP IS TO MAKE THE FIGURE 5.4 AND ALSO NEED TO SEE IF MULTIPLE LOOCV COULD WORK__
__QUESTION: THE LENGTH OF THE TRAIN INDEX IS SAME AS THE DATA, SO IS THE TEST DATA SET, IT SHOULD BE NOT RIGHT. __
```{r}
library(ISLR)
make_LOOCV_poly <- function(n, train_index){
  poly_temp <- NULL
  test_err <- NULL
  for (i in 1:n ) {
    temp.lm = lm(mpg ~ poly(horsepower, n), data = Auto, subset = train_index)
    temp.pre = predict(temp.lm, Auto[!train_index, ])
    print (length(temp.pre))
    test_err <- c(test_err, mean((Auto[!train_index, "mpg"] - temp.pre)^2))
    poly_temp <- c(poly_temp, i)
  }
  return (data.frame("poly_temp" = poly_temp, "test_err" = test_err))
}

train_index_0 <- sample(1:nrow(Auto), 1, replace = FALSE)
train_index <- sample(TRUE, size = nrow(Auto), replace = TRUE)
train_index[test_index_0] <- FALSE
nrow(Auto[train_index, ])
make_LOOCV_poly(10, test_index)
length(!train_index)
make_LOOCV_plot <- function() {
  for (i in 1:10){
    set.seed(i)
    train_index <- sample()
  }
}
```

note to color the work in the HTML
<span style="color:orange"> xxx </span>  
