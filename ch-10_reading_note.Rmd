---
title: "ch-10_reading_note.Rmd"
author: "wendy"
date: "12/10/2016"
output: html_document
---

| No. | SVM | Function Related |
| --- | --- | --- |
| 1 | PCA | prcomp(USArrests, scale = TRUE): scale = TRUE to have standard deviation one.; by default is to center variables to have mean zero; *k*th col is the *k*th principal component score vector. / biplot(pr.out, scale = 0); cumsum() to get the accumulative sum|
| 2 | Clustering(K-mean) | kmeans(x, 2, nstart = 20) |
| 3 | Clustering(Hierarchical Clustering) | hc.complete = hclust(dist(x), method = "complete"/ "average"/ "single")/ cutree(hc.complete, 2)/ scale()/ as.dist() correlation-based distance|

**Lab.4: Principal Components Analysis*
*purpose:* practice the PCA
*data:*   
*note:*  
*try-out:* 
```{r}
states = row.names(USArrests)
names(USArrests)

#check data basic information. 
apply(USArrests, 2, mean)
apply(USArrests, 2, var)

#do the PCA
pr.out = prcomp(USArrests, scale = TRUE)
names(pr.out)

pr.out$center
pr.out$scale
pr.out$rotation

dim(pr.out$x)

#plot the first two principal components
biplot(pr.out, scale = 0)
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale = 0)

# get the std
pr.out$sdev
pr.var = pr.out$sdev^2
pr.var

#get the proportion of var by each principla component
pve = pr.var/sum(pr.var)
pve

#plot the PVE
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = "b")
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", ylim = c(0, 1), type = "b")
```

**Lab.5.1: K-Mean Clustering*
*purpose:* 
*data:*   
*note:*  
*try-out:* 
```{r}
set.seed(2)
x = matrix(rnorm(50*2), ncol = 2)
x[1:25, 1] = x[1:25, 1] + 3
x[1:25, 2] = x[1:25, 2] -4

plot(x)

km.out = kmeans(x, 2, nstart = 20)
km.out$cluster
plot(x, col = (km.out$cluster + 1), main = "K-Means Clustering Results with K = 2", xlab = "", ylab = "", pch = 20, cex = 2)

# for the real data, could start with K = 3
set.seed(4)
km.out = kmeans(x, 3, nstart = 20)
km.out

plot(x, col = (km.out$cluster + 1), main = "K-Means Clustering Results with K = 3", xlab = "", ylab = "", pch = 20, cex = 2)

# about how the nstart value impact the result
set.seed(3)
km.out = kmeans(x, 3, nstart = 1)
km.out$tot.withinss

km.out = kmeans(x, 3, nstart = 29)
km.out$tot.withinss

km.out = kmeans(x, 3, nstart = 20)
km.out$tot.withinss

```

**Lab.5.2: Hierarchical Clustering*
*purpose:* 
*data:*   
*note:*  
*try-out:* 
```{r}
hc.complete = hclust(dist(x), method = "complete")
hc.average = hclust(dist(x), method = "average")
hc.single = hclust(dist(x), method = "single")

par(mfrow = c(1, 3))
plot(hc.complete, main = "Complete Linkage", xlab ="", sub = "", cex = .9)
plot(hc.average, main = "Average Linkage", xlab = "", sub = "", cex = .9)
plot(hc.single, main = "Single Linkage", xlab = "", sub = "", ce = .9)

# cut tree
cutree(hc.complete, 2)
cutree(hc.average, 2)
cutree(hc.single, 2)

# scale
par(mfrow = c(1, 1))
xsc = scale(x)
plot(hclust(dist(xsc), method = "complete"), main = "Hierarchical Clustering with Scaled Features")

# correlation-base distance
x = matrix(rnorm(30 * 3), ncol = 3)
dd = as.dist(1-cor(t(x)))
plot(hclust(dd, method = "complete"), main = "Complete Linkage with Correlation - Based Distance", xlab = "", sub = "")
```


**Lab.6: Data Example*
*purpose:* 
*data:*   
*note:*  
*try-out:* 
```{r}
library(ISLR)
nci.labs = NCI60$labs
nci.data = NCI60$data

dim(nci.data)

nci.labs[1:4]
table(nci.labs)

#PCA with the data
pr.out = prcomp(nci.data, scale = T)
Cols = function(vec){
  cols = rainbow(length(unique(vec)))
  return (cols[as.numeric(as.factor(vec))])
}

par(mfrow = c(1, 2))

# pca plot the 1st pca vs 2nd pca, and 1st pca vs 3rd pca
plot(pr.out$x[, 1:2], col = Cols(nci.labs), pch = 19, xlab = "Z1", ylab = "Z2")
plot(pr.out$x[, c(1, 3)], col = Cols(nci.labs), pch = 19, xlab = "Z1", ylab = "Z3")

# get and plot the pve
summary(pr.out)
par(mfrow = c(1, 1))
plot(pr.out)

par(mfrow = c(1, 2))
pve = 100 * pr.out$sdev ^ 2/sum(pr.out$sdev ^ 2)
plot(pve, type = "o", ylab = "PVE", xlab = "Principal Component", col = "blue")
plot(cumsum(pve), type = "o", ylab = "Cumulative PVE", xlab = "Principal Component", col = "brown3")

#Cluster(Hierarchical)
sd.data = scale(nci.data)
par(mfrow = c (1, 3))
data.dist = dist(sd.data)
plot(hclust(data.dist), labels = nci.labs, main = "Complete Linkage", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "average"), labels = nci.labs, main = "Average Linkage", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "single"), lables = nci.labs, main = "Singele Linkage", xlab = "", ylab = "", sub = "")

hc.out = hclust(dist(sd.data))
hc.clusters = cutree(hc.out, 4)
table(hc.clusters, nci.labs)

par(mfrow = c(1, 1))
plot(hc.out, labels = nci.labs)
abline(h = 139, col = "red")

hc.out

#cluster K-mean
set.seed(2)
km.out = kmeans(sd.data, 4, nstart = 20)
km.clusters = km.out $ cluster
table(km.clusters, hc.cluster)

# just perform the hierarchical clustering on the first few Principal component score vectors
hc.out = hclust(dist(pr.out$x[, 1:5]))
plot(hc.out, labels = nci.labs, main = "Hier. Clust. on First Five Score Vectors")
table(cutree(hc.out, 4), nci.labs)
```


stop in the page 404