---
title: "ch-9_reading_note.Rmd"
author: "wendy"
date: "12/8/2016"
output: html_document
---
####Purpose
support vector machine(SVM)  
way to decide which of the infinite possible separating hyperplanes

####Summary
| No. | SVM | relation/ merit | Flaws | Suggestion | Function Related |
| --- | --- | --- | --- | --- | --- |
| 1 | maximal margin classifier | simple and elegant | cannot be applied to most data sets, because it requires the classes be separable by a linear  boundary/ can overfit when p is large | --- | NA |
| 2 | support vector classifier (extension of the No. 1)/ soft margin classifier | greater robustness to individual observations & better classification of most of the training observation | just support the linear boundary | --- | under package of e1071: svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)/ cost: to specify the cost of a violation to the margin; scale = False: not to scale each feature to have mean zero ro standard deviation one/ svmfit$index get the support vectors/ tune(svm, y ~., ata = dat, kernel = "linear", ranges = list(cost = c(.001, .01, .1, 1, 5, 10, 100)): performs ten-fold cv on a set of models / tune.out$best.model could get the best model from the cv comparison|
| 3 | support vector machine (extension of the No.2) | accommodate non-linear class boundaries | --- | polynomial kernel of degree d/ radial kernel | svm() with kernel = "radial" or "polynomial"/ with gamma = xxx / under package of ROCR, help to show the ROC curve. but need to write a function first.|
| 4 | more than two classes (two ways): one-versus-one/ one-versus-all | intend for the binary classification setting | --- | not mentioned a lot in the ISLR | svm() |
| 5 | support vector machine and other statistical methods | --- | --- |when classes are well separated, SVM better; when class more overlapping, logistic regression better | --- |

**Lab.9.6.1: Support Vector Classifier*
*purpose:* apply the support vector classifier    
*data:* make it by user  
*note:* 
*try-out:* 
```{r}
par(mfrow = c(1, 1))
set.seed(1)
x = matrix(rnorm(20*2), ncol = 2)
y = c(rep(-1, 10), rep(1, 10))
x[y == 1, ] = x[y == 1, ] + 1
plot(x, col = (3-y))

library(e1071)
dat = data.frame(x = x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
plot(svmfit, dat)

svmfit$index

# smaller value of the cost parameter
svmfit = svm(y~., data = dat, kernel = "linear", cost = .1, scale = FALSE)
plot(svmfit, dat)
svmfit$index
summary(svmfit)

# use tune, which is cv
set.seed(1)
tune.out = tune(svm, y~., data = dat, kernel = "linear", ranges = list(cost = c(.001, .01, .1, 1.5, 10, 100)))

summary(tune.out)

bestmod = tune.out$best.model
summary(bestmod)

# predict function
xtest = matrix(rnorm(20 * 2), ncol = 2)
ytest = sample(c(-1, 1), 20, rep = TRUE)
xtest[ytest == 1, ] = xtest[ytest == 1, ] + 1
testdat = data.frame(x = xtest, y = as.factor(ytest))

ypred = predict(bestmod, testdat)
table(predict = ypred, truth = testdat$y)

# try the prediction with smaller cost
svmfit = svm(y~., data = dat, kernel = "linear", cost = .01, scale = FALSE)
ypred = predict(svmfit, testdat)
table(predict = ypred, truth = testdat$y)

# consider the linearly separable situation
x[y == 1, ] = x[y == 1, ] + .5
plot(x, col = (y + 5)/2, pch = 19)

dat = data.frame( x = x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 1e5)
summary(svmfit)
plot(svmfit, dat)

# try a smaller value of cost for the better fit
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 1)
summary(svmfit)
plot(svmfit, dat)
```

**Lab.9.6.2: Support Vector Machine*
*purpose:* apply the support vector machine    
*data:* make it by user  
*note:* 
*try-out:* 
```{r}
set.seed(1)
x = matrix ( rnorm(200 * 2), ncol = 2)
x[1:100, ] = x[1:100, ] + 2
x[101:150, ] = x[101:150, ] - 2
y = c(rep(1,150), rep(2, 50))
dat = data.frame(x = x, y = as.factor(y))

plot( x, col = y)

train = sample(200, 100)
svmfit = svm ( y ~ ., data = dat[train, ], kernel = "radial", gamma = 1, cost = 1)
plot(svmfit, dat[train, ])

summary(svmfit)

# larger value of cost, smaller training error, but may overfitting
svmfit = svm( y ~ ., data = dat[train, ], kernel = "radial", gamma = 1, cost = 1e5)
plot(svmfit, dat[train, ])

# use tune, determine the best choice of gamma and cost by cv
set.seed(1)
tune.out = tune(svm, y ~ ., data = dat[train, ], kernel = "radial", ranges = list(cost = c(.1, 1, 10, 100, 1000), gamma = c(.5, 1, 2, 3, 4)))
summary(tune.out)
tune.out$best.model

# predict / check the result 
table(true = dat[-train, "y"], pred = predict(tune.out$best.model, newdata = dat[-train, ]))
```

**Lab.9.6.3: ROC Curves*
*purpose:* try the ROC curves to make the figures 9.10 and 9.11 in the ISLR    
*data:*   
*note:* 
*try-out:* 
```{r}
library(ROCR)
rocplot = function(pred, truth, ...){
    predob = prediction(pred, truth)
    perf = performance(predob, "tpr", "fpr")
    plot(perf, ...)
}

svmfit.opt = svm(y~., data = dat[train, ], kernel = "radial", gamma = 2, cost = 1, decision.values = T)
fitted = attributes(predict(svmfit.opt, dat[train, ], decision.values = T))$decision.values

par(mfrow = c(1, 2))
rocplot(fitted, dat[train, "y"], main = "Training Data")

# changing the gamma to produce a more flexible fit and generate further improvements in accuracy
svmfit.flex = svm(y ~ ., data = dat[train, ], kernel = "radial", gamma = 50, cost = 1, decision.values = T)
fitted = attributes(predict(svmfit.flex, dat[train, ], decision.values = T))$decision.values
rocplot(fitted, dat[train, "y"], add = T, col = "red")

# try on the testing data
fitted = attributes(predict(svmfit.opt, dat[-train, ], decision.values = T))$decision.values
rocplot(fitted, dat[-train, "y"], main = "Test Data")
fitted = attributes(predict(svmfit.flex, dat[-train, ], decision.values = T))$decision.values
rocplot(fitted, dat[-train, "y"], add = T, col = "red")
```

**Lab.9.6.4: SVM w/ Multiple Classes*
*purpose:* one versus one approach
*data:*   
*note:* but still not sure how to do the one versus one 
*try-out:* 
```{r}
set.seed(1)
x = rbind(x, matrix(rnorm(50**2), ncol = 2))
y = c(y, rep(0, 50))
x[y == 0, 2] = x[ y == 0, 2] + 2
dat = data.frame(x = x, y = as.factor(y))

par(mfrow = c(1, 1))
plot(x, col = y+1)

svmfit = svm(y ~ ., data = dat, kernel = "radial", cost = 10, gamma = 1)
plot(svmfit, dat)
```


**Lab.9.6.5: Application to Gene Expression Data*
*purpose:* practice for multiple classes
*data:* Khan  
*note:*  
*try-out:* 
```{r}
library(ISLR)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)

table(Khan$ytrain)
table(Khan$ytest)

dat = data.frame(x = Khan$xtrain, y = as.factor(Khan$ytrain))
out = svm(y ~ ., data = dat, kernel = "linear", cost = 10)
summary(out)

table(out$fitted, dat$y)

dat.te = data.frame(x = Khan$xtest, y = as.factor(Khan$ytest))
pred.te = predict(out, newdata = dat.te)
table(pred.te, dat.te$y)
```
stop context at page 369
lab is from 377 about the support vector

####Remaining Questions

*still need to figure out how to make the ROC plot, not quite sure how the function work, which in the page 379, rocplot
*not know the theory neither the way to do the one-versus-one or one-versus-all